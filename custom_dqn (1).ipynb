{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_dqn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class DQN(nn.Module):\n",
        "  "
      ],
      "metadata": {
        "id": "hCs6hCxuNqRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import diambraArena\n",
        "from diambraArena.gymUtils import showGymObs\n",
        "\n",
        "settings = {}\n",
        "settings[\"romsPath\"] = \"/home/nabil/Downloads/\"\n",
        "settings[\"gameId\"] = \"doapp\"\n",
        "settings[\"actionSpace\"] = \"discrete\"\n",
        "settings[\"attackButCombination\"] = False # reduce action space size\n",
        "\n",
        "envId = \"TestEnv\"\n",
        "env = diambraArena.make(envId, settings)\n",
        "observation = env.reset()\n",
        "showGymObs(observation, env.charNames)"
      ],
      "metadata": {
        "id": "mte4zD9nNLha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "deep q net stuff\n"
      ],
      "metadata": {
        "id": "sc_UrC6vxs36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
      ],
      "metadata": {
        "id": "YWPotD7HxyIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[drawing upon architecture used here for feature extraction](https://www.youtube.com/watch?v=ynlNIfFl_9c)"
      ],
      "metadata": {
        "id": "5TSQ1pEMxzLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "\n",
        "\n",
        "class ImageNetwork(nn.Module):\n",
        "    # TO DO change architecutre ; input size\n",
        "    # image transforms\n",
        "    def __init__(self, image_width=480, image_height=512, num_actions = 16):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv_relu_stack = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, kernel_size = 5, stride=2),\n",
        "            nn.BatchNorm2d(32)\n",
        "            )\n",
        "            \n",
        "        def output_shape(size, kernel_size=5, stride=2):\n",
        "            return math.floor(((size - (kernel_size - 1) - 1)/stride) + 1)\n",
        "            \n",
        "        convw = output_shape(output_shape(output_shape(image_width)))\n",
        "        convh = output_shape(output_shape(output_shape(image_height)))\n",
        "        \n",
        "        self.get_actions = nn.Linear(convw*convh*32, num_actions)\n",
        "        \n",
        "\tdef forward(self, x):\n",
        "\t\tx = state\n",
        "\t\tif not isinstance(x, torch.Tensor):\n",
        "\t\tx = torch.tensor(x)\n",
        "\t\tx = self.linear_relu_stack(x)\n",
        "\t\tx = get_actions(x)\n",
        "\t\treturn x\n",
        "\t\t\n",
        "class PlayerObservationsNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Input: P1/P2 Health, Stage, P1/P2 Side\n",
        "    \"\"\"\n",
        "\tdef __init__(self, input_dim, output_dim):\n",
        "\t\tsuper(PlayerObservationsNet, self).__init__()\n",
        "\t\tself.linear_relu_stack = nn.Sequential(\n",
        "\t\t\tnn.Linear(input_dim, 32),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Linear(32,32),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Linear(32,output_dim)\n",
        "\t\t)\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.linear_relu_stack(x)\n",
        "\t\treturn x\n",
        "\t\t\n",
        "class GetActions(nn.Module):\n",
        "\t\"\"\"\n",
        "\tInput: outputs of the image and player-observations networks\n",
        "\t\"\"\"\n",
        "\tdef\n",
        "\n",
        "\n",
        "def player_obs_to_vec(observations):\n",
        "    \"\"\"\n",
        "    Process observations returned by env into a flat vector that can\n",
        "    be used by the PlayerObservationsNet\n",
        "    \"\"\"\n",
        "    \n",
        "n_actions = env.actions_space.n               \n",
        "image_net = ImageNetwork(num_actions = n_actions).to(device)\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "\n",
        "def predict(x):\n",
        "  \"\"\"\n",
        "  returns index (representing action) with greatest Q-value\n",
        "  \"\"\"\n",
        "  return torch.argmax(nn.functional.softmax(x,dim=0))\n",
        "    \n",
        "def loss_fcn(y, Q):\n",
        "    return nn.functional.mse_loss(y, Q)#.mean()\n",
        "\n",
        "opt = torch.optim.RMSprop(model.parameters(),lr=0.001)\n",
        "\n",
        "def update_weights(X, Y):\n",
        "    '''\n",
        "    input: nparray of indices of replay memory D to use as training samples\n",
        "    This updates Q Network parameters via gradient descent step\n",
        "    '''\n",
        "    opt.zero_grad()\n",
        "    loss = loss_fcn(X, Y)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        " "
      ],
      "metadata": {
        "id": "IC3Ko7-IxsFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**gameplay loop**"
      ],
      "metadata": {
        "id": "IqfxqX-rAhlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "def select_action(epsilon, observation):\n",
        "  \"\"\"\n",
        "  Input: epsilon - probability of choosing a random action\n",
        "  Output: Pick action according to probability below\n",
        "\n",
        "  P(pick random action) = epsilon\n",
        "  P(pick action with best Q value) = 1 - epsilon\n",
        "  \"\"\"\n",
        "    action_type = np.random.choice([0,1],p=[epsilon,1-epsilon])\n",
        "    if action_type == 0: # do random action\n",
        "        action = env.action_space.sample()\n",
        "    else: # do action with greatest Q-value\n",
        "        action = int(predict(theDeepQNet(observation))) # need to cast tensor to int\n",
        "\n",
        "memory = ReplayMemory(capacity=10000)\n",
        "epsilon = 0.9\n",
        "min_epsilon = 0.05\n",
        "decay_rate = 0.995 # for epsilon\n",
        "batch_size = 32\n",
        "gamma = 0.999\n",
        "\n",
        "theDeepQNet = NEURAL_NETWORK_THAT_GIVES_US_THE_PREDICTION_FOR_ACTIONS()\n",
        "state = observation\n",
        "while True:\n",
        "\n",
        "    epsilon = max(epsilon*decay_rate, min_epsilon)\n",
        "    \n",
        "    # need to preprocess the observation data before inputting to model\n",
        "    player_inp = player_obs_to_vec(observation) # preprocessed input\n",
        "    img_inp = img_obs_to_tensor(observation) # preprocessed input\n",
        "    input_for_model = CombinedNetwork(ImageNetwork(img_inp), PlayerNetwork(player_inp))\n",
        "\n",
        "    # change var name for model\n",
        "    predicted_move = thedeepQnet(input_for_model)\n",
        "    predicted_move = select_action(epsilon) \n",
        "\n",
        "    next_state, reward, done, info = env.step(predicted_move)\n",
        "    memory.push(state, action, reward, next_state)\n",
        "    state = next_state\n",
        "\n",
        "    train_indices = np.random.choice(np.arange(len(memory)),size=batch_size)\n",
        "    train_samples = [memory[i] for i in train_indices]\n",
        "\n",
        "    Q_values = [model(memory[i][0])[0][memory[i][1]] for i in train_indices] # list of tensors\n",
        "    expected_Q_values = [memory[i][2] + gamma*max(model(memory[i][3])[0]) for i in train_indices] # list of tensors\n",
        "    Q_values, expected_Q_values = torch.stack(Q_values), torch.stack(expected_Q_values)\n",
        "    update_weights(Q_values, expected_Q_values)\n",
        "\n",
        "    showGymObs(observation, env.charNames)\n",
        "    print(\"Reward: {}\".format(reward))\n",
        "    print(\"Done: {}\".format(done))\n",
        "    print(\"Info: {}\".format(info))\n",
        "\n",
        "    if done:\n",
        "        observation = env.reset()\n",
        "        showGymObs(observation, env.charNames)\n",
        "        break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "vwD44qZGAg7p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}