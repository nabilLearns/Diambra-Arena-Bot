{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIAMBRA_DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMV/soYtLX5KmTFz4uroh2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabilLearns/diambra-arena-bot/blob/master/Diambra_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import cv2 "
      ],
      "metadata": {
        "id": "zyn7fT-52E0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Q = Our net's output\n",
        "x = image\n",
        "a = action\n",
        "r = reward\n",
        "s = sequence\n",
        "phi = processed s\n",
        "gamma = discount factor\n",
        "D = replay buffer\n",
        "'''"
      ],
      "metadata": {
        "id": "v5XupCa42GYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJIMrkES2DVn"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "# need to rescale image, generate\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "Moves (0-8): (No-Move, Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left) \n",
        "Attacks (0-3): (No-Attack, Hold, Punch, Kick)\n",
        "\n",
        "\n",
        "EXAMPLE INPUT:\n",
        "actions = env.action_space\n",
        "observation, reward, done, info = env.step(actions)\n",
        "observation['frame'] = frame array shape = (480, 512, 3)\n",
        "observation['P1'] = {'oppChar1': 4, 'ownChar1': 8, 'oppChar': 4, 'ownChar': 8, 'oppHealth': 143, 'ownHealth': 187, 'oppSide': 0, 'ownSide': 1, 'oppWins': 1, 'ownWins': 0, 'actions': {'move': 5, 'attack': 7}}\n",
        "observation['stage'] = 1\n",
        "\n",
        "\n",
        "info = {'roundDone': False, 'stageDone': False, 'gameDone': False, 'epDone': False}\n",
        "Info: {'roundDone': False, 'stageDone': False, 'gameDone': False, 'epDone': False}\n",
        "observation[\"frame\"].shape: (480, 512, 3)\n",
        "observation[\"stage\"]: 1\n",
        "observation[\"P1\"][\"ownChar1\"]: Kasumi\n",
        "observation[\"P1\"][\"oppChar1\"]: Bass\n",
        "observation[\"P1\"][\"ownChar\"]: Kasumi\n",
        "observation[\"P1\"][\"oppChar\"]: Bass\n",
        "observation[\"P1\"][\"ownHealth\"]: 84\n",
        "observation[\"P1\"][\"oppHealth\"]: 23\n",
        "observation[\"P1\"][\"ownSide\"]: 1\n",
        "observation[\"P1\"][\"oppSide\"]: 1\n",
        "observation[\"P1\"][\"ownWins\"]: 0\n",
        "observation[\"P1\"][\"oppWins\"]: 0\n",
        "observation[\"P1\"][\"actions\"]: {'move': 6, 'attack': 0}\n",
        "Reward: 0.9423076923076923\n",
        "\n",
        "Done: False\n",
        "'''\n",
        "\n",
        "\n",
        "#In: 6 of (480, 512, 3)\n",
        "#To net: (6, 1, 256, 256)\n",
        "#Out of imagenet: (6, hidden_size)\n",
        "#Out of timenet: (hidden_size2)\n",
        "#Out of metanet: (hidden_size3)\n",
        "#Out of final net: one-val\n",
        "#One-val//8 = move, One-vack\n",
        "\n",
        "import cv2\n",
        "# GREYSCALE\n",
        "img = cv2.imread('sample_out_2.png')\n",
        "small = cv2.resize(img, (256,256)) \n",
        "img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "print(img_gray.shape)\n",
        "plt.imshow(img_gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image - Network\n",
        "class ImageNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    SAMPLE, NEED TO CHANGE\n",
        "    \"\"\"\n",
        "    def output_shape(size, kernel_size=5, stride=2):\n",
        "        return math.floor(((size - (kernel_size - 1) - 1)/stride) + 1)\n",
        "\n",
        "    def __init__(self, input_shape, hidden_size1):\n",
        "        super(ImageNetwork, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1)\n",
        "        )\n",
        "\n",
        "        convw, convh = input_shape[0], input_shape[1]\n",
        "        kernel_sizes = [8, 4, 3]\n",
        "        stride_vals = [4, 2, 1]\n",
        "        num_layers = 3\n",
        "        for i in range(num_layers):\n",
        "            convw = self.output_shape(convw, kernel_sizes[i], stride_vals[i])\n",
        "            convh = self.output_shape(convh, kernel_sizes[i], stride_vals[i])\n",
        "        convc = 64\n",
        "        dim_size = convc*convh*convw\n",
        "        \n",
        "        '''\n",
        "        (N, c_in, h_in, w_in)\n",
        "        (N, convc, convh, convw)\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        (N, X)\n",
        "        (N, hidden_size1)\n",
        "        '''\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(dim_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, hidden_size1)\n",
        "        )\n",
        "\n",
        "    def forward(self, in_vals):\n",
        "        out = self.conv(in_vals)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "3iAPv6ok2JBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeLSTM(nn.Module):\n",
        "    def __init__(self, input_shape, output_shape, hidden_size, num_layers):\n",
        "        super(TimeLSTM, self).__init__()\n",
        "        self.size_in = input_shape\n",
        "        self.size_out = output_shape\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.LSTM = nn.LSTM(input_size=self.size_in, hidden_size=self.hidden_size, num_layers=self.num_layers)\n",
        "        self.ac1 = nn.LeakyReLU()\n",
        "        self.FC = nn.Linear(in_features = self.hidden_size, out_features = self.size_out)\n",
        "        self.ac2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, in_vals):\n",
        "        out, (h_n, c_n) = self.LSTM(in_vals)\n",
        "        output = h_n[-1,:]\n",
        "        output = self.ac1(output)\n",
        "        output = self.FC(output)\n",
        "        output = self.ac2(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "f-_rgpOp2Kjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class metaData(nn.Module):\n",
        "    def __init__(self, input_shape, hidden_sz, n_outputs):\n",
        "        \"\"\"\n",
        "        input_shape = 12\n",
        "        \"\"\"\n",
        "        super(metaData, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(input_shape[0], hidden_sz),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_sz, hidden_sz) ,\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_sz, n_outputs)\n",
        "        )\n",
        "     \n",
        "    def forward(self, input):\n",
        "        return self.linear_relu_stack(input)  \n",
        "        \n",
        "        \n",
        "\n",
        "#observation['P1'] = {'oppChar1': 4, 'ownChar1': 8, 'oppChar': 4, 'ownChar': 8, 'oppHealth': 143, 'ownHealth': 187, 'oppSide': 0, 'ownSide': 1, 'oppWins': 1, 'ownWins': 0, 'actions': {'move': 5, 'attack': 7}}\n",
        "#observation['stage'] = 1\n",
        "        "
      ],
      "metadata": {
        "id": "lODZZInh2LIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class finalNetwork(nn.Module):\n",
        "    def __init__(self, input_shape, hidden_sz, n_actions=72):\n",
        "        super(finalNetwork, self).__init__()\n",
        "        self.dense_network = nn.Sequential(\n",
        "            nn.Linear(input_shape[0], hidden_sz),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_sz, n_actions)\n",
        "        )\n",
        "            \n",
        "    def forward(self, input):\n",
        "        return self.dense_network(input)\n",
        "        \n",
        "    def predict(self, input):  \n",
        "        return self.forward(input).argmax() "
      ],
      "metadata": {
        "id": "186bAPX-2ObU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(observation, past_5_frames):\n",
        "    \"\"\"given an observation, send it through the model pipeline and give us the best action\"\"\"\n",
        "    # PROCESS(observation[\"frame\"])\n",
        "        \n",
        "    current_frame = observation['frame']\n",
        "    metaData = observation['P1']\n",
        "\n",
        "    def get_input_to_metanet(observation):\n",
        "        lambda action_to_num: action['move']*8 + action['attack']\n",
        "        metaData = [value for key, value in observation['P1'].items() if not key == 'actions']\n",
        "        metaData += action_to_num(observation['P1']['actions'])  \n",
        "        metaData += observation['stage']  "
      ],
      "metadata": {
        "id": "4y7Zn7H-2PmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN Algorithm\n",
        "\n",
        "REPLAY_MEMORY = []\n",
        "\n",
        "def SELECT_ACTION(epsilon):\n",
        "    \"\"\"Selects random action w/ prob epsilon, otherwise picks the action of highest value from the Q-Network\"\"\"\n",
        "    \n",
        "    action_type = np.random.choice([0, 1], p=[epsilon, 1-epsilon])\n",
        "    if action_type == 0:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        input = preprocess(observation)\n",
        "        action = model.predict(input)f (observation)   # select best action from output of Q net-work\n",
        "\n",
        "def LOSS(y, Q):\n",
        "    return nn.functional.mse_loss(y, Q)\n",
        "\n",
        "def RUN_DQN_ALGORITHM(num_episodes, num_time_steps, eps=1, min_eps=0.05):\n",
        "    \"\"\"\n",
        "    Runs through DQN algo. as described in https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    eps (float): probability of selecting a random action. this can decay as the learning process progresses (i.e. do more exploration initially, then less later on) \n",
        "    \"\"\"\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        observation = env.reset()\n",
        "        for step in range(num_time_steps):\n",
        "            eps = max(eps*0.99, min_eps)\n",
        "            action = SELECT_ACTION(eps)\n",
        "            observation, reward, done, info = env.step(action)\n",
        "\n",
        "            # store transition in REPLAY_MEMORY\n",
        "            # sample random minibatch of transitions from REPLAY_MEMORY\n",
        "            # set target value y_i according to eqn in paper\n",
        "            # perform grad. desc. on LOSS(y_i, Q_value)\n",
        "\n",
        "\n",
        "\n",
        "transition = [state, action, reward, next_state]\n",
        "REPLAY_MEMORY.append(transition)"
      ],
      "metadata": {
        "id": "Y5u_S7mu2RD5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}